{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911835ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleVector:\n",
    "    def __init__(self, v_max=10, lowercase=True):\n",
    "        self.v_max = v_max\n",
    "        self.lowercase = lowercase\n",
    "        self.vocabulary = []\n",
    "        self.idf_scores = {}\n",
    "        \n",
    "    def fit(self, documents):\n",
    "        \n",
    "        # lowercase the documents \n",
    "        if self.lowercase:\n",
    "            documents = [doc.lower() for doc in documents]\n",
    "            \n",
    "        # split documents into words\n",
    "        tokenized_docs = [doc.split() for doc in documents]\n",
    "\n",
    "        # count word frequencies across all documents\n",
    "        word_counts = Counter(word for doc in tokenized_docs for word in doc)\n",
    "        \n",
    "        # build vocabulary (sorted by frequency, then alphabetically)\n",
    "        self.vocabulary = [word for word, _ in sorted(word_counts.items(), key=lambda x: (-x[1], x[0]))]\n",
    "        \n",
    "        # limit vocabulary to v_max most frequent terms\n",
    "        if self.v_max:\n",
    "            self.vocabulary = self.vocabulary[:self.v_max]\n",
    "            \n",
    "            \n",
    "        # calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        doc_freq = {word: 0 for word in self.vocabulary}\n",
    "\n",
    "        for doc in tokenized_docs:\n",
    "            for word in set(doc):  # Count each word only once per document\n",
    "                if word in self.vocabulary:\n",
    "                    doc_freq[word] += 1\n",
    "                    \n",
    "        self.idf_scores = {\n",
    "            word: math.log(1 + num_docs / (1 + freq)) + 1\n",
    "            for word, freq in doc_freq.items()\n",
    "        }\n",
    "        \n",
    "    def transform(self, documents):\n",
    "        \n",
    "        \n",
    "        if self.lowercase:\n",
    "            documents = [doc.lower() for doc in documents]\n",
    "\n",
    "        # split documents into words\n",
    "        tokenized_docs = [doc.split() for doc in documents]\n",
    "\n",
    "        # create TF-IDF vectors\n",
    "        tfidf_vectors = []\n",
    "        for doc in tokenized_docs:\n",
    "            word_counts = Counter(doc)\n",
    "            doc_length = len(doc)\n",
    "            tfidf_vector = []\n",
    "\n",
    "            for word in self.vocabulary:\n",
    "                tf = word_counts[word] / doc_length if doc_length > 0 else 0\n",
    "                idf = self.idf_scores.get(word, 0)\n",
    "                tfidf_vector.append(tf * idf)\n",
    "\n",
    "            tfidf_vectors.append(tfidf_vector)\n",
    "\n",
    "        return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa3e662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'is', 'the', 'baby', 'cat', 'fat', 'pitbull', 'with', 'and', 'flying']\n",
      "TF-IDF Vectors:\n",
      "[0.21164339756999317, 0.19847333311276488, 0.21164339756999317, 0.21164339756999317, 0.21164339756999317, 0.21164339756999317, 0.0, 0.23091223254840043, 0.0, 0.0]\n",
      "[0.24187816865142076, 0.22682666641458843, 0.24187816865142076, 0.24187816865142076, 0.0, 0.24187816865142076, 0.24187816865142076, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.35284148108935975, 0.3762549290133212, 0.0, 0.1881274645066606, 0.0, 0.1881274645066606, 0.0, 0.20525531782080036, 0.20525531782080036]\n",
      "[0.45150591481598545, 0.10585244432680793, 0.0, 0.11287647870399636, 0.11287647870399636, 0.11287647870399636, 0.11287647870399636, 0.24630638138496044, 0.12315319069248022, 0.12315319069248022]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "          'The cat is playing with a fat baby',\n",
    "          'The fat baby is fight a pitbull',\n",
    "          'The pitbull is bites the cat and is flying',\n",
    "          'A pilot is flying with a fat baby along with a cat and a pitbull'\n",
    "        ]\n",
    "\n",
    "# initialize the vectorizer\n",
    "vectorizer = SimpleVector(v_max=10, lowercase=True)\n",
    "\n",
    "# fit on the corpus\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# transform the corpus into TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.transform(corpus)\n",
    "\n",
    "# print the vocabulary and TF-IDF vectors\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary)\n",
    "print(\"TF-IDF Vectors:\")\n",
    "for vector in tfidf_vectors:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d30b4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with lowercase False\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleVector:\n",
    "    def __init__(self, v_max=10, lowercase=False):\n",
    "        self.v_max = v_max\n",
    "        self.lowercase = lowercase\n",
    "        self.vocabulary = []\n",
    "        self.idf_scores = {}\n",
    "        \n",
    "    def fit(self, documents):\n",
    "        \n",
    "        # lowercase the documents \n",
    "        if self.lowercase:\n",
    "            documents = [doc.lower() for doc in documents]\n",
    "            \n",
    "        # split documents into words\n",
    "        tokenized_docs = [doc.split() for doc in documents]\n",
    "\n",
    "        # count word frequencies across all documents\n",
    "        word_counts = Counter(word for doc in tokenized_docs for word in doc)\n",
    "        \n",
    "        # build vocabulary (sorted by frequency, then alphabetically)\n",
    "        self.vocabulary = [word for word, _ in sorted(word_counts.items(), key=lambda x: (-x[1], x[0]))]\n",
    "        \n",
    "        # limit vocabulary to v_max most frequent terms\n",
    "        if self.v_max:\n",
    "            self.vocabulary = self.vocabulary[:self.v_max]\n",
    "            \n",
    "            \n",
    "        # calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        doc_freq = {word: 0 for word in self.vocabulary}\n",
    "\n",
    "        for doc in tokenized_docs:\n",
    "            for word in set(doc):  # Count each word only once per document\n",
    "                if word in self.vocabulary:\n",
    "                    doc_freq[word] += 1\n",
    "                    \n",
    "        self.idf_scores = {\n",
    "            word: math.log(1 + num_docs / (1 + freq)) + 1\n",
    "            for word, freq in doc_freq.items()\n",
    "        }\n",
    "        \n",
    "    def transform(self, documents):\n",
    "        \n",
    "        \n",
    "        if self.lowercase:\n",
    "            documents = [doc.lower() for doc in documents]\n",
    "\n",
    "        # split documents into words\n",
    "        tokenized_docs = [doc.split() for doc in documents]\n",
    "\n",
    "        # create TF-IDF vectors\n",
    "        tfidf_vectors = []\n",
    "        for doc in tokenized_docs:\n",
    "            word_counts = Counter(doc)\n",
    "            doc_length = len(doc)\n",
    "            tfidf_vector = []\n",
    "\n",
    "            for word in self.vocabulary:\n",
    "                tf = word_counts[word] / doc_length if doc_length > 0 else 0\n",
    "                idf = self.idf_scores.get(word, 0)\n",
    "                tfidf_vector.append(tf * idf)\n",
    "\n",
    "            tfidf_vectors.append(tfidf_vector)\n",
    "\n",
    "        return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f368d1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'is', 'The', 'baby', 'cat', 'fat', 'pitbull', 'with', 'and', 'flying']\n",
      "TF-IDF Vectors:\n",
      "[0.21164339756999317, 0.19847333311276488, 0.21164339756999317, 0.21164339756999317, 0.21164339756999317, 0.21164339756999317, 0.0, 0.23091223254840043, 0.0, 0.0]\n",
      "[0.24187816865142076, 0.22682666641458843, 0.24187816865142076, 0.24187816865142076, 0.0, 0.24187816865142076, 0.24187816865142076, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.35284148108935975, 0.1881274645066606, 0.0, 0.1881274645066606, 0.0, 0.1881274645066606, 0.0, 0.20525531782080036, 0.20525531782080036]\n",
      "[0.3386294361119891, 0.10585244432680793, 0.0, 0.11287647870399636, 0.11287647870399636, 0.11287647870399636, 0.11287647870399636, 0.24630638138496044, 0.12315319069248022, 0.12315319069248022]\n"
     ]
    }
   ],
   "source": [
    "corpus =   [\n",
    "          'The cat is playing with a fat baby',\n",
    "          'The fat baby is fight a pitbull',\n",
    "          'The pitbull is bites the cat and is flying',\n",
    "          'A pilot is flying with a fat baby along with a cat and a pitbull'\n",
    "        ]\n",
    "# initialize the vectorizer\n",
    "vectorizer = SimpleVector(v_max=10, lowercase=False)\n",
    "\n",
    "# fit on the corpus\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# transform the corpus into TF-IDF vectors\n",
    "tfidf_vectors = vectorizer.transform(corpus)\n",
    "\n",
    "# print the vocabulary and TF-IDF vectors\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary)\n",
    "print(\"TF-IDF Vectors:\")\n",
    "for vector in tfidf_vectors:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d047b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2 extension with word removal\n",
    "\n",
    "class StopWordVector(SimpleVector):\n",
    "\n",
    "    def __init__(self, v_max=10, lowercase=True, remove_stopwords=False, stopwords=None):\n",
    "        \"\"\"\n",
    "        Initialize with optional stop-word removal.\n",
    "        \"\"\"\n",
    "        super().__init__(v_max, lowercase)\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stopwords = stopwords or [\"a\", \"able\", \"about\", \"across\", \"after\", \"all\", \"almost\", \"also\", \"am\", \"among\", \"an\", \"and\", \"any\", \"are\", \"as\",\n",
    "    \"at\", \"be\", \"because\", \"been\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"dear\", \"did\", \"do\", \"does\", \"either\", \"else\",\n",
    "    \"ever\", \"every\", \"for\", \"from\", \"get\", \"got\", \"had\", \"has\", \"have\", \"he\", \"her\", \"hers\", \"him\", \"his\", \"how\",\n",
    "    \"however\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"just\", \"least\", \"let\", \"like\", \"likely\", \"may\", \"me\",\n",
    "    \"might\", \"most\", \"must\", \"my\", \"neither\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"often\", \"on\", \"only\", \"or\", \"other\",\n",
    "    \"our\", \"own\", \"rather\", \"said\", \"say\", \"says\", \"she\", \"should\", \"since\", \"so\", \"some\", \"than\", \"that\", \"the\",\n",
    "    \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"this\", \"tis\", \"to\", \"too\", \"twas\", \"us\", \"wants\", \"was\", \"we\",\n",
    "    \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"would\", \"yet\", \"you\",\n",
    "    \"your\"]\n",
    "\n",
    "    def fit(self, documents):\n",
    "        \n",
    "        if self.lowercase:\n",
    "            documents = [doc.lower() for doc in documents]\n",
    "\n",
    "        # tokenize remove stop words\n",
    "        tokenized_docs = [doc.split() for doc in documents]\n",
    "        if self.remove_stopwords:\n",
    "            tokenized_docs = [[word for word in doc if word not in self.stopwords] for doc in tokenized_docs]\n",
    "\n",
    "        # build vocabulary and calculate frequencies\n",
    "        all_words = [word for doc in tokenized_docs for word in doc]\n",
    "        word_counts = Counter(all_words)\n",
    "\n",
    "        self.vocabulary = sorted(word_counts, key=lambda x: (-word_counts[x], x))\n",
    "        if self.v_max:\n",
    "            self.vocabulary = self.vocabulary[:self.v_max]\n",
    "\n",
    "        # calculate IDF scores\n",
    "        num_docs = len(documents)\n",
    "        doc_freq = {word: sum(1 for doc in tokenized_docs if word in doc) for word in self.vocabulary}\n",
    "        self.idf_scores = {word: math.log(1 + num_docs / (1 + freq)) + 1 for word, freq in doc_freq.items()}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd996c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['baby', 'cat', 'fat', 'pitbull', 'flying', 'along', 'bites', 'fight', 'pilot', 'playing']\n",
      "TF-IDF Vectors:\n",
      "[0.21164339756999317, 0.21164339756999317, 0.21164339756999317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26232653608351375]\n",
      "[0.24187816865142076, 0.0, 0.24187816865142076, 0.24187816865142076, 0.0, 0.0, 0.0, 0.2998017555240157, 0.0, 0.0]\n",
      "[0.0, 0.1881274645066606, 0.0, 0.1881274645066606, 0.20525531782080036, 0.0, 0.23317914318534555, 0.0, 0.0, 0.0]\n",
      "[0.11287647870399636, 0.11287647870399636, 0.11287647870399636, 0.11287647870399636, 0.12315319069248022, 0.13990748591120733, 0.0, 0.0, 0.13990748591120733, 0.0]\n"
     ]
    }
   ],
   "source": [
    "corpus =   [\n",
    "          'The cat is playing with a fat baby',\n",
    "          'The fat baby is fight a pitbull',\n",
    "          'The pitbull is bites the cat and is flying',\n",
    "          'A pilot is flying with a fat baby along with a cat and a pitbull'\n",
    "        ]\n",
    "\n",
    "vectorizer = StopWordVector(v_max=10, lowercase=True, remove_stopwords=True)\n",
    "vectorizer.fit(corpus)\n",
    "tfidf_vectors = vectorizer.transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary)\n",
    "print(\"TF-IDF Vectors:\")\n",
    "for vector in tfidf_vectors:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b831d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
